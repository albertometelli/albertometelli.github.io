@article{metelli2019policy,
  author    = {Alberto Maria Metelli and
               Guglielmo Manneschi and
               Marcello Restelli},
  title     = {Policy Space Identification in Configurable Environments},
  journal   = {CoRR},
  volume    = {abs/1909.03984},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.03984},
  archivePrefix = {arXiv},
  eprint    = {1909.03984},
  timestamp = {Tue, 17 Sep 2019 11:23:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1909-03984},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {We study the problem of identifying the policy space of a learning agent, having access to a set of demonstrations generated by its optimal policy. We introduce an approach based on statistical testing to identify the set of policy parameters the agent can control, within a larger parametric policy space. After presenting two identification rules (combinatorial and simplified), applicable under different assumptions on the policy space, we provide a probabilistic analysis of the simplified one in the case of linear policies belonging to the exponential family. To improve the performance of our identification rules, we frame the problem in the recently introduced framework of the Configurable Markov Decision Processes, exploiting the opportunity of configuring the environment to induce the agent revealing which parameters it can control. Finally, we provide an empirical evaluation, on both discrete and continuous domains, to prove the effectiveness of our identification rules.},
}

@article{metelli2018safe,
  author    = {Alberto Maria Metelli and
               Matteo Pirotta and
               Daniele Calandriello and
               Marcello Restelli},
  title     = {Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach},
  year      = {2019},
  abstract = {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy--iteration algorithms. When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur. To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy, until no improving policy can be found. We propose three safe policy--iteration schemata that differ in the way the next policy is chosen w.r.t. the estimated greedy policy. Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared on some chain-walk domains, the prison domain and on the Blackjack card game.},
  note = {Under revision for JMLR}
}

@article{likmeta2020dealing,
  author    = {Amarildo Likmeta and Alberto Maria Metelli and Giorgia Ramponi and Andrea Tirinzoni and Matteo Giuliani and Marcello Restelli},
  title     = {Dealing with Multiple Experts and Non-Stationarity in Inverse Reinforcement Learning},
  year      = {2020},
  note = {Under revision for MLJ, Special Issue on Reinforcement Learning for Real Life}
}
