@inproceedings{bianchi2017content,
 author = {Bianchi, Mattia and Cesaro, Federico and Ciceri, Filippo and Dagrada, Mattia and Gasparin, Alberto and Grattarola, Daniele and Inajjar, Ilyas and Metelli, Alberto Maria and Cella, Leonardo},
 title = {Content-Based Approaches for Cold-Start Job Recommendations},
 booktitle = {Proceedings of the Recommender Systems Challenge 2017},
 series = {RecSys Challenge '17},
 year = {2017},
 isbn = {978-1-4503-5391-5},
 location = {Como, Italy},
 pages = {6:1--6:5},
 articleno = {6},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/3124791.3124793},
 doi = {10.1145/3124791.3124793},
 acmid = {3124793},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {ACM RecSys Challenge 2017, Cold-Start recommendations, Content-Based Filtering, Job recommendations, Recommendation Systems},
 abstract = {This paper provides an overview of the approach we adopted as team Lunatic Goats for the ACM RecSys Challenge 2017 [7]. The competition, organized by XING.com, focuses on a cold start job recommendation scenario. The goal was to design and tune a recommendation system able to predict past usersâ€™ interactions, for the offline stage, and to provide recommendations pushed every day to real users through the XING portal, for the online stage. Our strategy, which saw models coming from different techniques combined in a multi-layer ensemble, granted us the first place in the offline part and the qualification as second best team in the final leaderboard. All our algorithms mainly resort to content-based approaches, that, thanks to its ability to provide good recommendations even for cold-start items allowed us, quite unexpectedly, to achieve good results in terms of prediction quality and computational time.}
}

@article{metelli2018configurableWorkshop,
  author    = {Alberto Maria Metelli and
               Mirco Mutti and
               Marcello Restelli},
  title     = {Configurable Markov Decision Processes},
  journal = {European Workshop on Reinforcement Learning 14 (EWRL 14)},
  year = {2018},
  abstract = {In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.},
  url       = {https://ewrl.files.wordpress.com/2018/09/ewrl_14_2018_paper_5.pdf},
}

@article{doro2019gradientWorkshop,
  author    = {Pierluca D'Oro and
               Alberto Maria Metelli and
               Andrea Tirinzoni and
               Matteo Papini and
               Marcello Restelli},
  title     = {Gradient-Aware Model-based Policy Search},
  journal = {Workshop on Meta-Learning (MetaLearn 2019) @NeurIPS 2019},
  abstract = {Traditional MBRL approaches learn a model of the environment dynamics without explicitly considering how it will be used by the agent. In the presence of misspecified model classes, this can lead to poor estimates, as some relevant information is ignored.  In this paper, we introduce a model-based policy search approach that, by meta-learning, exploits the knowledge of the current agent policy to learn an approximate transition model, focusing on the portions of the environment that are most relevant for policy improvement. We integrate gradient-aware model learning into a batch policy improvement algorithm, named Gradient-Aware Model-based Policy Search (GAMPS), which iteratively learns a transition model and uses it, together with the collected trajectories, to update the policy. Finally, we empirically validate GAMPS on benchmark domains analyzing and discussing its properties.},
  year = {2019},
}

@article{likmeta2020autonomous,
  author    = {Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano and Andrea Alessandretti},
  title     = {Autonomous Driving with Reinforcement Learning and Rule-based Policies},
  journal = {Workshop on AI for Autonomous Driving (AIAD) @ICML 2020},
  year = {2020},
  url = {https://drive.google.com/file/d/1ASJa-pOgZ_Z78KTVjrTV1kqqtQ5-RP_w/view},
  slides = {https://drive.google.com/file/d/1aoZQIPwX47DcedqZmqVyHexQkUAaoWMz/view}
}

