@article{metelli2019policy,
  author    = {Alberto Maria Metelli and
               Guglielmo Manneschi and
               Marcello Restelli},
  title     = {Policy Space Identification in Configurable Environments},
  journal   = {Machine Learning},
  year      = {2021},
  url={https://doi.org/10.1007/s10994-021-06033-3},
  doi={https://doi.org/10.1007/s10994-021-06033-3},
  abstract={We study the problem of identifying the policy space available to an agent in a learning process, having access to a set of demonstrations generated by the agent playing the optimal policy in the considered space. We introduce an approach based on frequentist statistical testing to identify the set of policy parameters that the agent can control, within a larger parametric policy space. After presenting two identification rules (combinatorial and simplified), applicable under different assumptions on the policy space, we provide a probabilistic analysis of the simplified one in the case of linear policies belonging to the exponential family. To improve the performance of our identification rules, we make use of the recently introduced framework of the Configurable Markov Decision Processes, exploiting the opportunity of configuring the environment to induce the agent to reveal which parameters it can control. Finally, we provide an empirical evaluation, on both discrete and continuous domains, to prove the effectiveness of our identification rules.},
  slides={https://albertometelli.github.io/files/slides_ecml2021.pdf},
  rankSJR={SJR 2020: \textbf{Q1}},
  rankCORE={CORE 2020: \textbf{A}},
  pages={1-53},
}
%talk={https://recorder-v3.slideslive.com/?share=47320&s=d2fd379b-5e1c-4be1-b800-0af3d80623bf},


@article{metelli2018safe,
  author    = {Alberto Maria Metelli and
               Matteo Pirotta and
               Daniele Calandriello and
               Marcello Restelli},
  title     = {Safe Policy Iteration: A Monotonically Improving Approximate Policy Iteration Approach},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {97},
  pages   = {1-83},
  url     = {http://jmlr.org/papers/v22/19-707.html},
  abstract = {This paper presents a study of the policy improvement step that can be usefully exploited by approximate policy--iteration algorithms. When either the policy evaluation step or the policy improvement step returns an approximated result, the sequence of policies produced by policy iteration may not be monotonically increasing, and oscillations may occur. To address this issue, we consider safe policy improvements, i.e., at each iteration we search for a policy that maximizes a lower bound to the policy improvement w.r.t. the current policy, until no improving policy can be found. We propose three safe policy--iteration schemata that differ in the way the next policy is chosen w.r.t. the estimated greedy policy. Besides being theoretically derived and discussed, the proposed algorithms are empirically evaluated and compared on some chain-walk domains, the prison domain and on the Blackjack card game.},
  rankSJR={SJR 2020: \textbf{Q1}},
  rankCORE={CORE 2020: \textbf{A*}},
}


@Article{likmeta2021dealing,
author={Likmeta, Amarildo
and Metelli, Alberto Maria
and Ramponi, Giorgia
and Tirinzoni, Andrea
and Giuliani, Matteo
and Restelli, Marcello},
title={Dealing with multiple experts and non-stationarity in inverse reinforcement learning: an application to real-life problems},
journal={Machine Learning},
abstract={In real-world applications, inferring the intentions of expert agents (e.g., human operators) can be fundamental to understand how possibly conflicting objectives are managed, helping to interpret the demonstrated behavior. In this paper, we discuss how inverse reinforcement learning (IRL) can be employed to retrieve the reward function implicitly optimized by expert agents acting in real applications. Scaling IRL to real-world cases has proved challenging as typically only a fixed dataset of demonstrations is available and further interactions with the environment are not allowed. For this reason, we resort to a class of truly batch model-free IRL algorithms and we present three application scenarios: (1) the high-level decision-making problem in the highway driving scenario, and (2) inferring the user preferences in a social network (Twitter), and (3) the management of the water release in the Como Lake. For each of these scenarios, we provide formalization, experiments and a discussion to interpret the obtained results.},
volume    = {110},
  number    = {9},
  pages     = {2541--2576},
  year      = {2021},
  url       = {https://doi.org/10.1007/s10994-020-05939-8},
  doi       = {https://doi.org/10.1007/s10994-020-05939-8},
  rankSJR={SJR 2020: \textbf{Q1}},
  rankCORE={CORE 2020: \textbf{A}},
}


@article{metelli2020importance,
  author  = {Alberto Maria Metelli and Matteo Papini and Nico Montali and Marcello Restelli},
  title   = {Importance Sampling Techniques for Policy Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {141},
  pages   = {1-75},
  url     = {http://jmlr.org/papers/v21/20-124.html},
  abstract={How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods.},
  rankSJR={SJR 2020: \textbf{Q1}},
  rankCORE={CORE 2020: \textbf{A*}},
}


@article{likmeta2020combining,
title = "Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving",
journal = "Robotics and Autonomous Systems",
volume = "131",
pages = "103568",
year = "2020",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2020.103568",
url = "https://doi.org/10.1016/j.robot.2020.103568",
author = "Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano",
keywords = "Autonomous driving, Decision making, Interpretability, Reinforcement learning, Parameter-based exploration",
abstract = "The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.",
rank="SJR 2020: \textbf{Q1}",
  rankSJR={SJR 2020: \textbf{Q1}},
  rankCORE={CORE 2020: B},
}

@article{metelli2019ongradient,
  author    = {Alberto Maria Metelli and
               Matteo Pirotta and
               Marcello Restelli},
  title     = {On the use of the policy gradient and Hessian in inverse reinforcement
               learning},
  journal   = {Intelligenza Artificiale},
  volume    = {14},
  number    = {1},
  pages     = {117--150},
  year      = {2020},
  url       = {https://doi.org/10.3233/IA-180011},
  doi       = {10.3233/IA-180011},
  abstract = {Reinforcement Learning (RL) is an effective approach to solve sequential decision making problems when the environment is equipped with a reward function to evaluate the agent’s actions. However, there are several domains in which a reward function is not available and difficult to estimate. When samples of expert agents are available, Inverse Reinforcement Learning (IRL) allows recovering a reward function that explains the demonstrated behavior. Most of the classic IRL methods, in addition to expert’s demonstrations, require sampling the environment to evaluate each reward function, that, in turn, is built starting from a set of engineered features. This paper is about a novel model-free IRL approach that does not require to specify a function space where to search for the expert’s reward function. Leveraging on the fact that the policy gradient needs to be zero for an optimal policy, the algorithm generates an approximation space for the reward function, in which a reward is singled out employing a second-order criterion. After introducing our approach for finite domains, we extend it to continuous ones. The empirical results, on both finite and continuous domains, show that the reward function recovered by our algorithm allows learning policies that outperform those obtained with the true reward function, in terms of learning speed.},
  rankSJR={SJR 2020: Q3},
  note={Invited publication as winner of the \quotes{Premio NeoLaureati Leonardo Lesmo 2018}}
}
