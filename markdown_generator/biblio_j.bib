@Article{likmeta2021dealing,
author={Likmeta, Amarildo
and Metelli, Alberto Maria
and Ramponi, Giorgia
and Tirinzoni, Andrea
and Giuliani, Matteo
and Restelli, Marcello},
title={Dealing with multiple experts and non-stationarity in inverse reinforcement learning: an application to real-life problems},
journal={Machine Learning},
year={2021},
month={Mar},
day={14},
abstract={In real-world applications, inferring the intentions of expert agents (e.g., human operators) can be fundamental to understand how possibly conflicting objectives are managed, helping to interpret the demonstrated behavior. In this paper, we discuss how inverse reinforcement learning (IRL) can be employed to retrieve the reward function implicitly optimized by expert agents acting in real applications. Scaling IRL to real-world cases has proved challenging as typically only a fixed dataset of demonstrations is available and further interactions with the environment are not allowed. For this reason, we resort to a class of truly batch model-free IRL algorithms and we present three application scenarios: (1) the high-level decision-making problem in the highway driving scenario, and (2) inferring the user preferences in a social network (Twitter), and (3) the management of the water release in the Como Lake. For each of these scenarios, we provide formalization, experiments and a discussion to interpret the obtained results.},
issn={1573-0565},
doi={10.1007/s10994-020-05939-8},
url={https://doi.org/10.1007/s10994-020-05939-8}
}


@article{metelli2020importance,
  author  = {Alberto Maria Metelli and Matteo Papini and Nico Montali and Marcello Restelli},
  title   = {Importance Sampling Techniques for Policy Optimization},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {141},
  pages   = {1-75},
  url     = {http://jmlr.org/papers/v21/20-124.html},
  abstract={How can we effectively exploit the collected samples when solving a continuous control task with Reinforcement Learning? Recent results have empirically demonstrated that multiple policy optimization steps can be performed with the same batch by using off-distribution techniques based on importance sampling. However, when dealing with off-distribution optimization, it is essential to take into account the uncertainty introduced by the importance sampling process. In this paper, we propose and analyze a class of model-free, policy search algorithms that extend the recent Policy Optimization via Importance Sampling (Metelli et al., 2018) by incorporating two advanced variance reduction techniques: per-decision and multiple importance sampling. For both of them, we derive a high-probability bound, of independent interest, and then we show how to employ it to define a suitable surrogate objective function that can be used for both action-based and parameter-based settings. The resulting algorithms are finally evaluated on a set of continuous control tasks, using both linear and deep policies, and compared with modern policy optimization methods.}
}

@article{likmeta2020combining,
title = "Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving",
journal = "Robotics and Autonomous Systems",
volume = "131",
pages = "103568",
year = "2020",
issn = "0921-8890",
doi = "https://doi.org/10.1016/j.robot.2020.103568",
url = "http://www.sciencedirect.com/science/article/pii/S0921889020304085",
author = "Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano",
keywords = "Autonomous driving, Decision making, Interpretability, Reinforcement learning, Parameter-based exploration",
abstract = "The design of high-level decision-making systems is a topical problem in the field of autonomous driving. In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques."
}

@article{metelli2019ongradient,
  author    = {Alberto Maria Metelli and
               Matteo Pirotta and
               Marcello Restelli},
  title     = {On the use of the policy gradient and Hessian in inverse reinforcement
               learning},
  journal   = {Intelligenza Artificiale},
  volume    = {14},
  number    = {1},
  pages     = {117--150},
  year      = {2020},
  url       = {https://doi.org/10.3233/IA-180011},
  doi       = {10.3233/IA-180011},
  timestamp = {Mon, 05 Oct 2020 09:36:33 +0200},
  biburl    = {https://dblp.org/rec/journals/ia/MetelliPR20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  abstract = {Reinforcement Learning (RL) is an effective approach to solve sequential decision making problems when the environment is equipped with a reward function to evaluate the agent’s actions. However, there are several domains in which a reward function is not available and difficult to estimate. When samples of expert agents are available, Inverse Reinforcement Learning (IRL) allows recovering a reward function that explains the demonstrated behavior. Most of the classic IRL methods, in addition to expert’s demonstrations, require sampling the environment to evaluate each reward function, that, in turn, is built starting from a set of engineered features. This paper is about a novel model-free IRL approach that does not require to specify a function space where to search for the expert’s reward function. Leveraging on the fact that the policy gradient needs to be zero for an optimal policy, the algorithm generates an approximation space for the reward function, in which a reward is singled out employing a second-order criterion. After introducing our approach for finite domains, we extend it to continuous ones. The empirical results, on both finite and continuous domains, show that the reward function recovered by our algorithm allows learning policies that outperform those obtained with the true reward function, in terms of learning speed.}
}
