@incollection{liotet2022lifelong,
  author    = {Pierre Liotet and
  		Francesco Vidaich and
  		Alberto Maria Metelli and
  		Marcello Restelli},
  title={Lifelong Hyper-Policy Optimization with Multiple Importance Sampling Regularization},
  booktitle = {The Thirty-Sixth {AAAI} Conference on Artificial Intelligence ({AAAI})},
  publisher = {{AAAI} Press},
  year      = {2022}, 
  url       = {https://arxiv.org/abs/2112.06625},
  abstract = {Learning in a lifelong setting, where the dynamics continually evolve, is a hard challenge for current reinforcement learning algorithms. Yet this would be a much needed feature for practical applications. In this paper, we propose an approach which learns a hyper-policy, whose input is time, that outputs the parameters of the policy to be queried at that time. This hyper-policy is trained to maximize the estimated future performance, efficiently reusing past data by means of importance sampling, at the cost of introducing a controlled bias. We combine the future performance estimate with the past performance to mitigate catastrophic forgetting. To avoid overfitting the collected data, we derive a differentiable variance bound that we embed as a penalization term. Finally, we empirically validate our approach, in comparison with state-of-the-art algorithms, on realistic environments, including water resource management and trading.},
  acceptance={Acceptance rate: 1349/9020 (15.0\%)},
  rankCORE={CORE 2021: \textbf{A*}},
  rankGGS={GGS 2021: \textbf{A++}},
  note={To appear}
}

@incollection{metelli2021subgaussian,
author    = {Alberto Maria Metelli and
               Alessio Russo and
               Marcello Restelli},
title={Subgaussian and Differentiable Importance Sampling for Off-Policy Evaluation and Learning},
  booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS)},
  year      = {2021},
  acceptance={Acceptance rate: 2344/9122 (25.7\%), Spotlight: 260/9122 (2.9\%)},
  abstract={Importance Sampling (IS) is a widely used building block for a large variety of off-policy estimation and learning algorithms. However, empirical and theoretical studies have progressively shown that vanilla IS leads to poor estimations whenever the behavioral and target policies are too dissimilar. In this paper, we analyze the theoretical properties of the IS estimator by deriving a novel anticoncentration bound that formalizes the intuition behind its undesired behavior. Then, we propose a new class of IS transformations, based on the notion of power mean. To the best of our knowledge, the resulting estimator is the first to achieve, under certain conditions, two key properties: (i) it displays a subgaussian concentration rate; (ii) it preserves the differentiability in the target distribution. Finally, we provide numerical simulations on both synthetic examples and contextual bandits, in comparison with off-policy evaluation and learning baselines.},
  url={https://proceedings.neurips.cc/paper/2021/hash/4476b929e30dd0c4e8bdbcc82c6ba23a-Abstract.html},
  code={https://github.com/albertometelli/subgaussian-is},
  poster={https://albertometelli.github.io/files/poster_neurips2021.pdf},
  slides={https://albertometelli.github.io/files/slides_neurips2021.pdf},
  rankCORE={CORE 2021: \textbf{A*}},
  rankGGS={GGS 2021: \textbf{A++}},
}
%talk={https://recorder-v3.slideslive.com/?share=49177&s=8022dc6d-03b0-41ac-b5bf-d818871bd57b},


@incollection{metelli2021learning,
author    = {Giorgia Ramponi and
			Alberto Maria Metelli and
               Alessandro Concetti and
               Marcello Restelli},
  title={Learning in Non-Cooperative Configurable Markov Decision Processes},
  booktitle = {Advances in Neural Information Processing Systems 34 (NeurIPS)},
  year      = {2021},
  acceptance={Acceptance rate: 2344/9122 (25.7\%)},
  abstract={The Configurable Markov Decision Process framework includes two entities: a Reinforcement Learning agent and a configurator that can modify some environmental parameters to improve the agent's performance. This presupposes that the two actors have the same reward functions. What if the configurator does not have the same intentions as the agent? This paper introduces the Non-Cooperative Configurable Markov Decision Process, a setting that allows having two (possibly different) reward functions for the configurator and the agent. Then, we consider an online learning problem, where the configurator has to find the best among a finite set of possible configurations. We propose two learning algorithms to minimize the configurator's expected regret, which exploits the problem's structure, depending on the agent's feedback. While a naive application of the UCB algorithm yields a regret that grows indefinitely over time, we show that our approach suffers only bounded regret. Furthermore, we empirically show the performance of our algorithm in simulated domains.},
  url={https://proceedings.neurips.cc/paper/2021/hash/c0f52c6624ae1359e105c8a5d8cd956a-Abstract.html},
  code={},
  poster={https://albertometelli.github.io/files/poster_neurips2021conf.png},
  slides={},
  talk={},
  rankCORE={CORE 2021: \textbf{A*}},
  rankGGS={GGS 2021: \textbf{A++}},
}
 

@incollection{metelli2021provably,
  author    = {Alberto Maria Metelli* and
               Giorgia Ramponi* and
               Alessandro Concetti and
               Marcello Restelli},
  title     = {Provably Efficient Learning of Transferable Rewards},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning ({ICML})},
  volume    = {139},
  pages     = {7665--7676},
  publisher = {{PMLR}},
  year      = {2021},
  acceptance={Acceptance rate: 1184/5513 (21.5\%)},
  abstract = {The reward function is widely accepted as a succinct, robust, and transferable representation of a task. Typical approaches, at the basis of Inverse Reinforcement Learning (IRL), leverage on expert demonstrations to recover a reward function. In this paper, we study the theoretical properties of the class of reward functions that are compatible with the expert’s behavior. We analyze how the limited knowledge of the expert’s policy and of the environment affects the reward reconstruction phase. Then, we examine how the error propagates to the learned policy’s performance when transferring the reward function to a different environment. We employ these findings to devise a provably efficient active sampling approach, aware of the need for transferring the reward function, that can be paired with a large variety of IRL algorithms. Finally, we provide numerical simulations on benchmark environments.} ,
  url       = {http://proceedings.mlr.press/v139/metelli21a.html},
  poster = {https://albertometelli.github.io/files/poster_icml2021.png},
  slides={https://albertometelli.github.io/files/slides_icml2021.pdf},
  talk={https://slideslive.com/38959627},
  rankCORE={CORE 2021: \textbf{A*}},
  rankGGS={GGS 2021: \textbf{A++}},
}


@incollection{metelli2021policy,
  author    = {Alberto Maria Metelli* and
               Matteo Papini* and
               Pierluca D'Oro and
               Marcello Restelli},
  title={Policy Optimization as Online Learning with Mediator Feedback},
  booktitle = {The Thirty-Fifth {AAAI} Conference on Artificial Intelligence ({AAAI})},
  publisher = {{AAAI} Press},
  year      = {2021}, 
  pages     = {8958--8966},
  abstract = {Policy Optimization (PO) is a widely used approach to address continuous control tasks. In this paper, we introduce the notion of mediator feedback that frames PO as an online learning problem over the policy space. The additional available information, compared to the standard bandit feedback, allows reusing samples generated by one policy to estimate the performance of other policies. Based on this observation, we propose an algorithm, RANDomized-exploration policy Optimization via Multiple Importance Sampling with Truncation (RANDOMIST), for regret minimization in PO, that employs a randomized exploration strategy, differently from the existing optimistic approaches. When the policy space is finite, we show that under certain circumstances, it is possible to achieve constant regret, while always enjoying logarithmic regret. We also derive problem-dependent regret lower bounds. Then, we extend RANDOMIST to compact policy spaces. Finally, we provide numerical simulations on finite and compact policy spaces, in comparison with PO and bandit baselines.},
  url       = {https://ojs.aaai.org/index.php/AAAI/article/view/17083},
  poster={https://albertometelli.github.io/files/poster_aaai2021.pdf},
  slides={https://albertometelli.github.io/files/slides_aaai2021.pdf},
  supplementary={https://arxiv.org/abs/2012.08225},
  talk={https://slideslive.com/38949290},
  code={https://github.com/proceduralia/randomist},
  acceptance={Acceptance rate: 1692/7911 (21.4\%)},
  rankCORE={CORE 2021: \textbf{A*}},
  rankGGS={GGS 2021: \textbf{A++}},
}
%rank={GGS 2021: \textbf{A++}, CORE 2021: \textbf{A*}},

@incollection{metelli2020control,
  author    = {Alberto Maria Metelli and
               Flavio Mazzolini and
               Lorenzo Bisi and
               Luca Sabbioni and
               Marcello Restelli},
  title     = {Control Frequency Adaptation via Action Persistence in Batch Reinforcement Learning},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning ({ICML})},
  volume    = {119},
  pages     = {6862--6873},
  publisher = {{PMLR}},
  year      = {2020},
  abstract = {The choice of the control frequency of a system has a relevant impact on the ability of reinforcement learning algorithms to learn a highly performing policy. In this paper, we introduce the notion of action persistence  that consists in the repetition of an action for a fixed number of decision steps, having the effect of modifying the control frequency. We start analyzing how action persistence affects the performance of the optimal policy, and then we present a novel algorithm, Persistent Fitted Q-Iteration (PFQI), that extends FQI, with the goal of learning the optimal value function at a given persistence. After having provided a theoretical study of PFQI and a heuristic approach to identify the optimal persistence, we present an experimental campaign on benchmark domains to show the advantages of action persistence and proving the effectiveness of our persistence selection method.},
  url       = {http://proceedings.mlr.press/v119/metelli20a.html},
  code={https://github.com/albertometelli/pfqi},
  slides={https://albertometelli.github.io/files/slides_icml2020.pdf},
  talk={https://slideslive.com/38927876},
  acceptance={Acceptance rate: 1088/4990 (21.8\%)},
  rankCORE={CORE 2020: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}


@inproceedings{ramponi2020truly,
  title = 	 {Truly Batch Model-Free Inverse Reinforcement Learning about Multiple Intentions},
  author = 	 {Ramponi, Giorgia and Likmeta, Amarildo and Metelli, Alberto Maria and Tirinzoni, Andrea and Restelli, Marcello},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics ({AISTATS})},
  pages = 	 {2359--2369},
  year = 	 {2020},
  volume = 	 {108},
  publisher = 	 {PMLR},
  abstract = 	 {We consider Inverse Reinforcement Learning (IRL) about multiple intentions, \ie the problem of estimating the unknown reward functions optimized by a group of experts that demonstrate optimal behaviors. Most of the existing algorithms either require access to a model of the environment or need to repeatedly compute the optimal policies for the hypothesized rewards. However, these requirements are rarely met in real-world applications, in which interacting with the environment can be expensive or even dangerous. In this paper, we address the IRL about multiple intentions in a fully model-free and batch setting. We first cast the single IRL problem as a constrained likelihood maximization and then we use this formulation to cluster agents based on the likelihood of the assignment. In this way, we can efficiently solve, without interactions with the environment, both the IRL and the clustering problem. Finally, we evaluate the proposed methodology on simulated domains and on a real-world social-network application.},
  url = 	 {http://proceedings.mlr.press/v108/ramponi20a.html},
  code={https://github.com/gioramponi/sigma-girl-MIIRL},
  talk={https://slideslive.com/38930164},
  rankCORE={CORE 2020: \textbf{A}},
  rankGGS={GGS 2018: \textbf{A+}},
}
%rank={GGS 2021: \textbf{A+}, CORE 2021: \textbf{A}},

@inproceedings{doro2020gradient,
  author    = {Pierluca D'Oro* and
               Alberto Maria Metelli* and
               Andrea Tirinzoni and
               Matteo Papini and
               Marcello Restelli},
  title     = {Gradient-Aware Model-Based Policy Search},
  booktitle = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence ({AAAI})},
  pages     = {3801--3808},
  year      = {2020},
  abstract = {Traditional model-based reinforcement learning approaches learn a model of the environment dynamics without explicitly considering how it will be used by the agent. In the presence of misspecified model classes, this can lead to poor estimates, as some relevant available information is ignored. In this paper, we introduce a novel model-based policy search approach that exploits the knowledge of the current agent policy to learn an approximate transition model, focusing on the portions of the environment that are most relevant for policy improvement. We leverage a weighting scheme, derived from the minimization of the error on the model-based policy gradient estimator, in order to define a suitable objective function that is optimized for learning the approximate transition model. Then, we integrate this procedure into a batch policy improvement algorithm, named Gradient-Aware Model-based Policy Search (GAMPS), which iteratively learns a transition model and uses it, together with the collected trajectories, to compute the new policy parameters. Finally, we empirically validate GAMPS on benchmark domains analyzing and discussing its properties.},
  acceptance={Acceptance rate: 1591/7737 (20.6\%)},
  url={https://doi.org/10.1609/aaai.v34i04.5791},
  supplementary={https://arxiv.org/abs/1909.04115},
  rankCORE={CORE 2020: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}

@inproceedings{metelli2019propagating,
  author    = {Alberto Maria Metelli* and
               Amarildo Likmeta* and
               Marcello Restelli},
  title     = {Propagating Uncertainty in Reinforcement Learning via Wasserstein Barycenters},
  booktitle = {Advances in Neural Information Processing Systems 32 ({NeurIPS})},
  year      = {2019},
  pages     = {4335--4347},
  abstract = {How does the uncertainty of the value function propagate when performing temporal difference learning? In this paper, we address this question by proposing a Bayesian framework in which we employ approximate posterior distributions to model the uncertainty of the value function and Wasserstein barycenters to propagate it across state-action pairs. Leveraging on these tools, we present an algorithm, Wasserstein Q-Learning (WQL), starting in the tabular case and then, we show how it can be extended to deal with continuous domains. Furthermore, we prove that, under mild assumptions, a slight variation of WQL enjoys desirable theoretical properties in the tabular setting. Finally, we present an experimental campaign to show the effectiveness of WQL on finite problems, compared to several RL algorithms, some of which are specifically designed for exploration, along with some preliminary results on Atari games.},
  acceptance={Acceptance rate: 428/6743 (21.2\%)},
  url={https://papers.nips.cc/paper/8685-propagating-uncertainty-in-reinforcement-learning-via-wasserstein-barycenters},
  poster = {https://albertometelli.github.io/files/poster_neurips2019.pdf},
  code = {https://github.com/albertometelli/wql},
  rankCORE={CORE 2018: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}

@inproceedings{beraha2019feature,
  author    = {Mario Beraha and
               Alberto Maria Metelli and
               Matteo Papini and
               Andrea Tirinzoni and
               Marcello Restelli},
  title     = {Feature Selection via Mutual Information: New Theoretical Insights},
  booktitle = {International Joint Conference on Neural Networks ({IJCNN})},
  organization={IEEE},
  pages     = {1--9},
  year      = {2019},
  url       = {https://doi.org/10.1109/IJCNN.2019.8852410},
  doi       = {10.1109/IJCNN.2019.8852410},
  abstract = {Mutual information has been successfully adopted in filter feature-selection methods to assess both the relevancy of a subset of features in predicting the target variable and the redundancy with respect to other variables. However, existing algorithms are mostly heuristic and do not offer any guarantee on the proposed solution. In this paper, we provide novel theoretical results showing that conditional mutual information naturally arises when bounding the ideal regression/classification errors achieved by different subsets of features. Leveraging on these insights, we propose a novel stopping condition for backward and forward greedy methods which ensures that the ideal prediction error using the selected feature subset remains bounded by a user-specified threshold. We provide numerical simulations to support our theoretical claims and compare to common heuristic methods.},
  rankCORE={CORE 2018: A},
  rankGGS={GGS 2018: B},
}

@inproceedings{metelli2019reinforcement,
  author    = {Alberto Maria Metelli and
               Emanuele Ghelfi and
               Marcello Restelli},
  title     = {Reinforcement Learning in Configurable Continuous Environments},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning ({ICML})},
  volume    = {97},
  pages     = {4546--4555},
  year      = {2019},
  publisher = 	 {PMLR},
  abstract = {Configurable Markov Decision Processes (Conf-MDPs) have been recently introduced as an extension of the usual MDP model to account for the possibility of configuring the environment to improve the agent’s performance. Currently, there is still no suitable algorithm to solve the learning problem for real-world Conf-MDPs. In this paper, we fill this gap by proposing a trust-region method, Relative Entropy Model Policy Search (REMPS), able to learn both the policy and the MDP configuration in continuous domains without requiring the knowledge of the true model of the environment. After introducing our approach and providing a finite-sample analysis, we empirically evaluate REMPS on both benchmark and realistic environments by comparing our results with those of the gradient methods.},
  acceptance = {Acceptance rate: 773/3424 (22.6\%)},
  url       = {http://proceedings.mlr.press/v97/metelli19a.html},
  code = {https://github.com/albertometelli/remps},
  poster = {https://albertometelli.github.io/files/icml2019-remps/poster.pdf},
  slides = {https://albertometelli.github.io/files/icml2019-remps/slides.pdf},
  rankCORE={CORE 2018: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}

@inproceedings{papini2019optimistic,
  author    = {Matteo Papini and
               Alberto Maria Metelli and
               Lorenzo Lupo and
               Marcello Restelli},
  title     = {Optimistic Policy Optimization via Multiple Importance Sampling},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning ({ICML})},
  volume    = {97},
  pages     = {4989--4999},
  year      = {2019},
  publisher = 	 {PMLR},
  abstract = {Policy Search (PS) is an effective approach to Reinforcement Learning (RL) for solving
control tasks with continuous state-action spaces. In this paper, we address the exploration-exploitation trade-off in PS by proposing an approach based on Optimism in the Face of Uncertainty. We cast the PS problem as a suitable Multi Armed Bandit (MAB) problem, defined over the policy parameter space, and we propose a class of algorithms that effectively exploit the problem structure, by leveraging Multiple Importance Sampling to perform an off-policy estimation of the expected return.
We show that the regret of the proposed approach is bounded by $\widetilde{\mathcal{O}}(\sqrt{T})$ for both discrete and continuous parameter spaces. Finally, we evaluate our algorithms on tasks of varying difficulty, comparing them with existing MAB and RL algorithms.},
  acceptance = {Acceptance rate: 773/3424 (22.6\%)},
  url       = {http://proceedings.mlr.press/v97/papini19a.html},
  code = {https://github.com/WolfLo/optimist},
  poster = {https://albertometelli.github.io/files/poster_optimist.pdf},
  slides = {https://icml.cc/media/Slides/icml/2019/104(11-14-00)-11-14-25-5158-optimistic_poli.pdf},
  talk = {https://slideslive.com/38917404},
  rankCORE={CORE 2018: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}

@inproceedings{metelli2018policy,
  author    = {Alberto Maria Metelli and
               Matteo Papini and
               Francesco Faccio and
               Marcello Restelli},
  title     = {Policy Optimization via Importance Sampling},
  booktitle = {Advances in Neural Information Processing Systems 31 ({NeurIPS})},
  pages     = {5447--5459},
  year      = {2018},
  abstract = {Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.},
  acceptance = {Acceptance rate: 1011/4856 (20.8\%), Oral: 30/4856 (0.62\%)},
  url       = {http://papers.nips.cc/paper/7789-policy-optimization-via-importance-sampling},
  code = {https://github.com/T3p/pois},
  poster = {https://albertometelli.github.io/files/poster_nips2018.pdf},
  slides = {https://albertometelli.github.io/files/talk_nips2018.pdf},
  rankCORE={CORE 2018: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}

@inproceedings{metelli2018configurable,
  author    = {Alberto Maria Metelli* and
               Mirco Mutti* and
               Marcello Restelli},
  title     = {Configurable Markov Decision Processes},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning ({ICML})},
  volume    = {80},
  pages     = {3488--3497},
  year      = {2018},
  abstract = {In many real-world problems, there is the possibility to configure, to a limited extent, some environmental parameters to improve the performance of a learning agent. In this paper, we propose a novel framework, Configurable Markov Decision Processes (Conf-MDPs), to model this new type of interaction with the environment. Furthermore, we provide a new learning algorithm, Safe Policy-Model Iteration (SPMI), to jointly and adaptively optimize the policy and the environment configuration. After having introduced our approach and derived some theoretical results, we present the experimental evaluation in two explicative problems to show the benefits of the environment configurability on the performance of the learned policy.},
  acceptance = {Acceptance rate: 618/2473 (25.0\%)},
  url       = {http://proceedings.mlr.press/v80/metelli18a.html},
  code = {https://github.com/albertometelli/Configurable-Markov-Decision-Processes-ICML-2018},
  poster = {https://albertometelli.github.io/files/poster_icml2018.pdf},
  slides = {https://albertometelli.github.io/files/slides_icml2018.pdf},
  rankCORE={CORE 2018: \textbf{A*}},
  rankGGS={GGS 2018: \textbf{A++}},
}


@inproceedings{metelli2017compatible,
  author    = {Alberto Maria Metelli and
               Matteo Pirotta and
               Marcello Restelli},
  title     = {Compatible Reward Inverse Reinforcement Learning},
  booktitle = {Advances in Neural Information Processing Systems 30 ({NIPS})},
  pages     = {2047--2056},
  year      = {2017},
  abstract = {Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. This paper is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert's reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert's policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is empirically compared to other IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian (LQG) and Car on the Hill environments.},
  acceptance = {Acceptance rate: 678/3240 (20.9\%)},
  url       = {http://papers.nips.cc/paper/6800-compatible-reward-inverse-reinforcement-learning},
  code = {https://github.com/albertometelli/crirl},
  poster = {https://albertometelli.github.io/files/poster_nips2017.pdf},
  rankCORE={CORE 2017: \textbf{A*}},
  rankGGS={GGS 2017: \textbf{A++}},
}
