

<!doctype html>
<html lang="en" class="no-js">
  <head>
    

<meta charset="utf-8">



<!-- begin SEO -->









<title>Compatible reward inverse reinforcement learning - Alberto Maria Metelli, Ph.D.</title>







<meta property="og:locale" content="en-US">
<meta property="og:site_name" content="Alberto Maria Metelli, Ph.D.">
<meta property="og:title" content="Compatible reward inverse reinforcement learning">


  <link rel="canonical" href="https://albertometelli.github.io/publication/0501-2017-Compatible-reward-inverse-reinforcement-learning">
  <meta property="og:url" content="https://albertometelli.github.io/publication/0501-2017-Compatible-reward-inverse-reinforcement-learning">



  <meta property="og:description" content="Abstract  Sequential decision making problems arise in a variety of areas in Artificial Intelligence. Reinforcement Learning proposes a number of algorithms able to learn an optimal behavior by interacting with the environment. The major assumption is that the learning agent receives a reward as soon as an action is performed. However, there are several application domains in which a reward function is not available and difficult to estimate, but samples of expert agents playing an optimal policy are simple to generate. Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. Most of the classic IRL methods, in addition to expert&#39;s demonstrations, require sampling the environment in order to compute the optimal policy for each candidate reward function. Furthermore, in most of the cases, it is necessary to specify a priori a set of engineered features that the algorithms combine to single out the reward function. This thesis is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert&#39;s reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert&#39;s policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is compared to state-of-the-art IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian Regulator and Car on the Hill environments. The empirical results show that the reward function recovered by our algorithm allows learning policies that outperform both behavioral cloning and those obtained with the true reward function, in terms of learning speed. ">





  

  





  <meta property="og:type" content="article">
  <meta property="article:published_time" content="2017-07-01T00:00:00+02:00">








  <script type="application/ld+json">
    {
      "@context" : "http://schema.org",
      "@type" : "Person",
      "name" : "Alberto Maria Metelli",
      "url" : "https://albertometelli.github.io",
      "sameAs" : null
    }
  </script>






<!-- end SEO -->


<link href="https://albertometelli.github.io/feed.xml" type="application/atom+xml" rel="alternate" title="Alberto Maria Metelli, Ph.D. Feed">

<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="https://albertometelli.github.io/assets/css/main.css">

<meta http-equiv="cleartype" content="on">
    

<!-- start custom head snippets -->

<link rel="apple-touch-icon" sizes="57x57" href="https://albertometelli.github.io/images/apple-touch-icon-57x57.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="60x60" href="https://albertometelli.github.io/images/apple-touch-icon-60x60.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="72x72" href="https://albertometelli.github.io/images/apple-touch-icon-72x72.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="76x76" href="https://albertometelli.github.io/images/apple-touch-icon-76x76.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="114x114" href="https://albertometelli.github.io/images/apple-touch-icon-114x114.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="120x120" href="https://albertometelli.github.io/images/apple-touch-icon-120x120.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="144x144" href="https://albertometelli.github.io/images/apple-touch-icon-144x144.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="152x152" href="https://albertometelli.github.io/images/apple-touch-icon-152x152.png?v=M44lzPylqQ">
<link rel="apple-touch-icon" sizes="180x180" href="https://albertometelli.github.io/images/apple-touch-icon-180x180.png?v=M44lzPylqQ">
<link rel="icon" type="image/png" href="https://albertometelli.github.io/images/favicon-32x32.png?v=M44lzPylqQ" sizes="32x32">
<link rel="icon" type="image/png" href="https://albertometelli.github.io/images/android-chrome-192x192.png?v=M44lzPylqQ" sizes="192x192">
<link rel="icon" type="image/png" href="https://albertometelli.github.io/images/favicon-96x96.png?v=M44lzPylqQ" sizes="96x96">
<link rel="icon" type="image/png" href="https://albertometelli.github.io/images/favicon-16x16.png?v=M44lzPylqQ" sizes="16x16">
<link rel="manifest" href="https://albertometelli.github.io/images/manifest.json?v=M44lzPylqQ">
<link rel="mask-icon" href="https://albertometelli.github.io/images/safari-pinned-tab.svg?v=M44lzPylqQ" color="#000000">
<link rel="shortcut icon" href="/images/favicon.ico?v=M44lzPylqQ">
<meta name="msapplication-TileColor" content="#000000">
<meta name="msapplication-TileImage" content="https://albertometelli.github.io/images/mstile-144x144.png?v=M44lzPylqQ">
<meta name="msapplication-config" content="https://albertometelli.github.io/images/browserconfig.xml?v=M44lzPylqQ">
<meta name="theme-color" content="#ffffff">
<link rel="stylesheet" href="https://albertometelli.github.io/assets/css/academicons.css"/>

<script type="text/x-mathjax-config"> MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); </script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/latest.js?config=TeX-MML-AM_CHTML' async></script>

<!-- end custom head snippets -->

  </head>

  <body>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->
    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        <button><div class="navicon"></div></button>
        <ul class="visible-links">
          <li class="masthead__menu-item masthead__menu-item--lg"><a href="https://albertometelli.github.io/">Alberto Maria Metelli, Ph.D.</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://albertometelli.github.io/about/">About Me</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://albertometelli.github.io/news/">News</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://albertometelli.github.io/publications/">Publications</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://albertometelli.github.io/teaching/">Teaching</a></li>
          
            
            <li class="masthead__menu-item"><a href="https://albertometelli.github.io/contacts/">Contacts</a></li>
          
        </ul>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>

    





<div id="main" role="main">
  


  <div class="sidebar sticky">
  



<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
<div itemscope itemtype="http://schema.org/Person">

  <div class="author__avatar">
    
    	<img src="https://albertometelli.github.io/images/profile3.png" class="author__avatar" alt="Alberto Maria Metelli">
    
  </div>

  <div class="author__content">
    <h3 class="author__name">Alberto Maria Metelli</h3>
    <p class="author__bio">Postdoctoral Researcher at Politecnico di Milano</p>
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li><i class="fa fa-fw fa-map-marker" aria-hidden="true"></i> Milan, Italy</li>
      
      
      
      
      
        <li><a href="https://www.deib.polimi.it/eng/people/details/926910"><i class="fa fa-university"></i> Institutional Page</a></li>
      
      
        <li><a href="https://www.linkedin.com/in/alberto-maria-metelli-01504214b"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> LinkedIn</a></li>
      
      
        <li><a href="https://twitter.com/alberto_metelli"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
      
      
        <li><a href="https://github.com/albertometelli"><i class="fab fa-fw fa-github" aria-hidden="true"></i> Github</a></li>
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
      
        <li><a href="https://scholar.google.it/citations?user=R31IsPwAAAAJ&hl=en"><i class="ai ai-google-scholar ai"></i> Google Scholar</a></li>
      
      <!--
      
        <li><a href="https://www.researchgate.net/profile/Alberto_Maria_Metelli"><i class="ai ai-researchgate ai" aria-hidden="true"></i> ResearchGate</a></li>
      
      
      
        <li><a href="https://orcid.org/0000-0002-3424-5212"><i class="ai ai-orcid ai"></i> ORCID</a></li>
      
      
      
	  
        <li><a href="https://dblp.org/pid/209/4941.html"><i class="ai ai-dblp ai"></i> dblp</a></li>
      
      
        <li><a href="https://www.scopus.com/authid/detail.uri?authorId=57195947711"><i class="ai ai-scopus ai"></i> Scopus</a></li>
      
      -->
    </ul>
  </div>
</div>

  
  </div>


  <article class="page" itemscope itemtype="http://schema.org/CreativeWork">
    <meta itemprop="headline" content="Compatible reward inverse reinforcement learning">
    <meta itemprop="description" content="Abstract  Sequential decision making problems arise in a variety of areas in Artificial Intelligence. Reinforcement Learning proposes a number of algorithms able to learn an optimal behavior by interacting with the environment. The major assumption is that the learning agent receives a reward as soon as an action is performed. However, there are several application domains in which a reward function is not available and difficult to estimate, but samples of expert agents playing an optimal policy are simple to generate. Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. Most of the classic IRL methods, in addition to expert&#39;s demonstrations, require sampling the environment in order to compute the optimal policy for each candidate reward function. Furthermore, in most of the cases, it is necessary to specify a priori a set of engineered features that the algorithms combine to single out the reward function. This thesis is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert&#39;s reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert&#39;s policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is compared to state-of-the-art IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian Regulator and Car on the Hill environments. The empirical results show that the reward function recovered by our algorithm allows learning policies that outperform both behavioral cloning and those obtained with the true reward function, in terms of learning speed. ">
    <meta itemprop="datePublished" content="July 01, 2017">
    

    <div class="page__inner-wrap">
      
        <header>
          <h1 class="page__title" itemprop="headline">Compatible reward inverse reinforcement learning
</h1>
          
        
        
        
			<p><b> Alberto Maria Metelli</b></p> 
		
        
        
          <p><i>Politecnico di Milano</i>, 2017.
		  
		  </p>
          
          <p>
          
          
          
          
          
          
          
          </p>
          
          
        
        
             
        <!--
          <p>Recommended citation:  Alberto Maria Metelli&quot;Compatible reward inverse reinforcement learning.&quot; Politecnico di Milano, 2017 <a href="http://hdl.handle.net/10589/135141"><u>http://hdl.handle.net/10589/135141</u></a></p>
        -->
    
        </header>
      

      <section class="page__content" itemprop="text">
        <p>Abstract
 <br /> Sequential decision making problems arise in a variety of areas in Artificial Intelligence. Reinforcement Learning proposes a number of algorithms able to learn an optimal behavior by interacting with the environment. The major assumption is that the learning agent receives a reward as soon as an action is performed. However, there are several application domains in which a reward function is not available and difficult to estimate, but samples of expert agents playing an optimal policy are simple to generate. Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. Most of the classic IRL methods, in addition to expert's demonstrations, require sampling the environment in order to compute the optimal policy for each candidate reward function. Furthermore, in most of the cases, it is necessary to specify a priori a set of engineered features that the algorithms combine to single out the reward function. This thesis is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert's reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert's policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is compared to state-of-the-art IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian Regulator and Car on the Hill environments. The empirical results show that the reward function recovered by our algorithm allows learning policies that outperform both behavioral cloning and those obtained with the true reward function, in terms of learning speed. <br /></p>

<p>[<a href="http://hdl.handle.net/10589/135141" target="_blank">Link</a>] [<a href="/files/bibtex/mastersthesis.bib" target="_blank">BibTeX</a>]</p>
<pre> @mastersthesis{mastersthesis,
    author = "Metelli, Alberto Maria",
    title = "Compatible reward inverse reinforcement learning",
    school = "Politecnico di Milano",
    year = "2017",
    month = "July",
    doi = "http://hdl.handle.net/10589/135141",
    keywords = "Reinforcement Learning, Inverse Reinforcement Learning, Policy Gradient, Feature Extraction",
    supervisor = "Prof. Marcello Restelli (Politecnico di Milano, Italy)",
    url = "http://hdl.handle.net/10589/135141"
} </pre>

        
      </section>
      
      <footer class="page__meta">
        
        




      </footer>

      

      


  <nav class="pagination">
    
      <a href="https://albertometelli.github.io/publication/0205-2017-Content-Based-Approaches-for-Cold-Start-Job-Recommendations" class="pagination--pager" title="Content-Based Approaches for Cold-Start Job Recommendations
">Previous</a>
    
    
      <a href="https://albertometelli.github.io/publication/0001-2018-Configurable-Markov-Decision-Processes" class="pagination--pager" title="Configurable Markov Decision Processes
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
</div>


    <div class="page__footer">
      <footer>
        <!-- start custom footer snippets -->
<!--<a href="/sitemap/">Sitemap</a> -->
<!-- end custom footer snippets -->

        

<div class="page__footer-copyright">&copy; 2022 Alberto Maria Metelli. Powered by <a href="http://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://github.com/academicpages/academicpages.github.io">AcademicPages</a>, a fork of <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    <script src="https://albertometelli.github.io/assets/js/main.min.js"></script>




  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', '', 'auto');
  ga('send', 'pageview');
</script>






  </body>
</html>

