---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
<!--<img src='/images/profile.png' width='300' align='middle'/>-->
Since March 2021, I am Postdoctoral Research Assistant at the Dipartimento di Elettronica, Informazione e Bioingegneria 
([DEIB](https://www.deib.polimi.it/)), in the Artificial Intelligence and Robotic Laboratory ([AIRLab](http://airlab.deib.polimi.it/)) 
of [Politecnico di Milano](https://www.polimi.it/). In March 2021, I received the  Ph.D. in Information Technology at Politecnico di 
Milano (with honors), defending the dissertation "[Exploiting Environment Configurability in Reinforcement Learning](https://www.politesi.polimi.it/handle/10589/170616)" ([Slides](https://albertometelli.github.io/files/slides_phd_thesis.pdf)),
under the supervision of Prof. [Marcello Restelli](http://home.deib.polimi.it/restelli/MyWebSite/index.shtml). 
In July 2015, I obtained the Bachelor of Science in Computer Engineering at Politecnico di Milano (with honors) and in July 2017, 
I received the Master of Science in Computer Science and Engineering at Politecnico di Milano (with honors), 
defending the thesis "[Compatible Reward Inverse Reinforcement Learning](https://www.politesi.polimi.it/handle/10589/135141)". 

Download my [Curriculum Vitae](/files/cv.pdf)
<!--<a class="btn btn-primary btn-lg" href="/files/cv.pdf" role="button">Get my CV</a>-->

Research Interests
---
My main research interests revolve around Artificial Intelligence and Machine Learning, in particular <b>Reinfocement Learning</b>. I am currently working on
reinforcement learning in configurable environments, off-policy reinforcement learning, and the applications to autonomous driving. I am also interested in algorithms, optimization, and recommender systems.

<!---
Major Research topic
---
<h3>Reinforcement Learning in Configurable Markov Decision Processes</h3>
<i>Markov Decision Processes</i> (MDPs) are a popular formalism to model sequential decision-making problems. Solving an MDP means to find a policy, i.e., a prescription of actions, which maximizes a given utility function. In classical <i>Reinforcement Learning</i> (RL) framework the MDP parameters are assumed to be fixed, unknown and out of the control of the agent. However, there exist several real-world scenarios in which the environment is <i>partially controllable</i> and, therefore, it might be beneficial to configure some of its features. For instance, a human car driver has at her/his disposal a number of possible vehicle configurations she/he can act on (eg., seasonal tires, stability and vehicle attitude, engine model, automatic speed control, parking aid system) to improve the driving style or speed up the process of learning a good driving policy. 
In this research, we introduce a novel framework to model <i>Configurable Markov Decision Processes</i> (Conf-MDPs), i.e., MDPs that admit the possibility to alter some environmental parameters to a limited extent. At an intuitive level, there exists a tight connection between environment, policy and learning process. First, in some contexts, the agent is allowed to select the task to solve within a given set. In this case, it is beneficial to configure the environment in order to identify the MDP maximizing the performance of the optimal policy. Second, even when the task is fixed, it might be convenient to dynamically change the environment (eg., the reward function or the discount factor) in order to ease the learning process, speeding up convergence to the optimal policy. For both cases, we will start with the formulation of RL in the CMDP framework, we will derive learning algorithms capable of taking advantage of the environment configurability and study their theoretical properties. Then, we will compare our algorithms with the state-of-the-art methods, especially with reward shaping and intrinsically motivated learning, in some relevant application like vehicle configuration or teaching planning.
-->

<!--
Reading Group RL POLIMI
---
Together with [Giorgia Ramponi](https://gioramponi.github.io/), I co-organize the [Reading Group RL POLIMI](https://sites.google.com/view/rgrlpolimi/).
-->
